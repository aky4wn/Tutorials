{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Latent Dirichlet Allocation\n",
    "\n",
    "As a final, more advanced, example of variational inference, we can consider Latent Dirichlet Allocation (LDA).  LDA is a Bayesian topic model that uses variational inference in the original paper to perform posterior inference for the parameters.  It is a very widely used model and many extensions to more complicated settings, such as dynamic topic models, also exist.\n",
    "\n",
    "The data generating process described for the original LDA model (http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf) is as follows:\n",
    "\n",
    "1. Choose the number of topics N, $N\\sim\\mbox{Poisson}(\\epsilon)$.\n",
    "\n",
    "2. Choose topic level parameters $\\theta$, $\\theta\\sim\\mbox{Dirichlet}(\\alpha)$.\n",
    "\n",
    "3. For each of the $n$ words $w_n$:\n",
    "    \n",
    "    a. Choose a topic $z_n\\sim\\mbox{Multinomial}(\\theta)$\n",
    "    \n",
    "    b. Choose a word $w_n$ from a multinomial probability conditioned on the topic $z_n$, $p(w_n | z_n, \\beta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The joint distribution of the model is then\n",
    "\n",
    "$$p(\\theta, z, w, |\\alpha, \\beta) = p(\\theta | \\alpha)\\prod_{n=1}^N p(z_n | \\theta)p(w_n |z_n, \\beta).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters of interest exist at several levels in the document corpus.  At the lowest level, the parameters $z_{dn}$ and $w_{dn}$ are at the word-level and are sampled once for each word $n$ in each document $d$.  At the document level are the parameters $\\theta_d$, which are sampled once for each document $d$. Finally, at the overall corpus level are the parameters $\\alpha$ and $\\beta$, which are sampled once to generate the corpus of documents.\n",
    "\n",
    "The variational distribution for inference is\n",
    "\n",
    "$$q(\\theta, z |\\gamma, \\phi) = q(\\theta|\\gamma)\\prod_{n=1}^N q(z_n |\\phi_n).$$\n",
    "\n",
    "We can also obtain empirical Bayes estimates for $\\alpha$ and $\\beta$ by maximizing the variational lower bound for these parameters.\n",
    "\n",
    "Full derivations for all steps of the inference can be found in the original paper (http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf).  A summary of the inference steps is presented here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $k$ be the dimension of the topic parameter $z_n$, so $k$ is the number of topics and is specified ahead of the inference.  $V$ is the total number of words in the vocabulary across the corpus. There are assumed to be $M$ documents in the corpus and $N$ words in each document, where $N$ can be sampled from a Poisson distribution as described in the data generating process above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variational E-Step:** The goal for the E-step is to find the optimizing values of the variational parameters, $\\gamma_d^*$ and $\\phi_d^*$ for each document $d$:\n",
    "\n",
    "1. Initialize $\\phi_{ni} = \\dfrac{1}{k}$ for all $i$ and $n$\n",
    "2. Initialize $\\gamma_i = \\alpha_i + \\dfrac{N}{k}$ for all $i$\n",
    "3. Until convergence, repeat for $n = 1, \\ldots, N$:\n",
    "\n",
    "    a. For $i = 1, \\ldots, k$: $$\\phi_{ni} = \\beta_{iw_n}\\mbox{exp}(\\psi(\\gamma_i))$$\n",
    "        \n",
    "    b. Normalize $\\phi_n$ to sum to 1\n",
    "    \n",
    "    c. $$\\gamma = \\alpha + \\sum_{n=1}^N\\phi_n$$\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variational M-Step:** Goal: update the parameters $\\alpha$ and $\\beta$.\n",
    "\n",
    "\\begin{aligned}\n",
    "\\beta_{ij} &\\propto \\sum_{d=1}^M\\sum_{n=1}^{N_d}\\phi_{ni}w_{dn}^j \\\\\n",
    "\\alpha_{new} = \\alpha_{old} - H(\\alpha_{old})^{-1}g(\\alpha_{old}) \\\\\n",
    "(H^{-1}g)_i &= \\dfrac{g_i - c}{h_i} \\\\\n",
    "c &= \\dfrac{\\sum_{j=1}^k g_j/h_j}{z^{-1} + \\sum h_j^{-1}} \\\\\n",
    "g_j &= M\\left(\\psi\\left(\\sum_{i=1}^k\\alpha_i\\right) - \\psi(\\alpha_j)\\right) + \\sum_{d=1}^M\\left(\\psi(\\gamma_{di}) - \\psi\\left(\\sum_{j=1}^k \\gamma_{dj}\\right)\\right) \\\\\n",
    "h_j &= M\\psi'(\\alpha_j) \\\\\n",
    "z &= \\psi'\\left(\\sum_{j=1}^k \\alpha_j\\right)\n",
    "\\end{aligned}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implementation\n",
    "\n",
    "We will implement LDA for some toy data. Below are four sentences, each of which is treated as a document. There are then four documents in this toy corpus.  \n",
    "\n",
    "Note: The code is implemented for clarity and not for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "s1 = \"The quick brown fox\"\n",
    "s2 = \"The brown fox jumps over the fence\"\n",
    "s3 = \"The grey elephant sleeps.\"\n",
    "s4 = \"The dog, peacock, lion, tiger and elephant are friends.\"\n",
    "\n",
    "docs = collections.OrderedDict()\n",
    "docs[\"s1\"] = s1\n",
    "docs[\"s2\"] = s2\n",
    "docs[\"s3\"] = s3\n",
    "docs[\"s4\"] = s4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import stop_words\n",
    "import numpy as np\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from scipy import special\n",
    "\n",
    "\n",
    "#Function to make document, word matricies for LDA - performs stemming and tokenization#\n",
    "def make_word_matrix(corpus, needToSplit):\n",
    "    \n",
    "    #define dictionary to store \"cleaned\" words\n",
    "    cleanCorpus = {}\n",
    "    #Define stop words\n",
    "    stopWords = get_stop_words('english')\n",
    "    \n",
    "    \n",
    "    #Initialize stemmer\n",
    "    p_stemmer = PorterStemmer()\n",
    "\n",
    "    \n",
    "    #Define list to store corpus data#\n",
    "    c = []\n",
    "    #Define list to store order of words for each document#\n",
    "    wordOrder = []\n",
    "    #Define table to remove punctuation\n",
    "    table = dict.fromkeys(map(ord, string.punctuation))\n",
    "\n",
    "    M = len(corpus)\n",
    "    \n",
    "    #Check to make sure that dictionary isn't empty#\n",
    "    if M ==0:\n",
    "        print(\"Input dictionary is empty\")\n",
    "        return;\n",
    "    \n",
    "    removePunc = string.punctuation\n",
    "    #For each document in docs, caculate frequency of the words#\n",
    "    for i in corpus:\n",
    "        \n",
    "        #if the documents in the corpus are contained in a single string\n",
    "        if needToSplit == 1:\n",
    "            #Remove punctuation \n",
    "            text = corpus[i].translate(table)\n",
    "            #Splits string by blankspace and goes to lower case#\n",
    "            words = text.lower().split()\n",
    "        \n",
    "        else:\n",
    "            #Remove punctuation\n",
    "            for j in range(0, len(removePunc)):\n",
    "                while removePunc[j] in corpus[i]: \n",
    "                    corpus[i].remove(removePunc[j])    \n",
    "            \n",
    "            #convert everything to a lower case\n",
    "            corpus[i] = list(map(lambda x:x.lower(),corpus[i]))\n",
    "            words = corpus[i]\n",
    "\n",
    "        #Remove stop words#\n",
    "        text = [word for word in words if word not in stopWords]\n",
    "        # stem tokens\n",
    "        text = [p_stemmer.stem(i) for i in text]\n",
    "        #Find total number of words in each document#\n",
    "        N = len(text)\n",
    "        cleanCorpus[i] = text\n",
    "        #Find number of unique words in each document#\n",
    "        Vwords = list(set(text))\n",
    "        wordOrder.append(Vwords)\n",
    "\n",
    "    #Find unique words in the corpus, this is the vocabulary#    \n",
    "    wordOrder = list(set(x for l in wordOrder for x in l))\n",
    "    wordOrder = sorted(wordOrder)\n",
    "    #Find the number of unique words in the corpus vocabulary#\n",
    "    V = len(wordOrder)\n",
    "    \n",
    "    #For each document in docs, caculate frequency of the words#\n",
    "    for i in cleanCorpus:\n",
    "        text = cleanCorpus[i]\n",
    "        N = len(text)\n",
    "        #Create matrix to store words for each document#\n",
    "        wordsMat = np.zeros((N, V), dtype = int)\n",
    "        count = 0\n",
    "        for word in text:\n",
    "            #Find which word in vocabulary current word in document corresponds to#\n",
    "            v = wordOrder.index(word)\n",
    "            #Add a 1 to that column in the wordsMat matrix#\n",
    "            wordsMat[count, v] = 1\n",
    "            count += 1\n",
    "        c.append(wordsMat)\n",
    "\n",
    "    return [c, wordOrder, M]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "         [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       "  array([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       "  array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]]),\n",
       "  array([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "         [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]])],\n",
       " ['brown',\n",
       "  'dog',\n",
       "  'eleph',\n",
       "  'fenc',\n",
       "  'fox',\n",
       "  'friend',\n",
       "  'grey',\n",
       "  'jump',\n",
       "  'lion',\n",
       "  'peacock',\n",
       "  'quick',\n",
       "  'sleep',\n",
       "  'tiger'],\n",
       " 4]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testMat = make_word_matrix(docs, 1)\n",
    "testMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Perform LDA E-step for one document\n",
    "## K is the number of topics\n",
    "## N is the number of words in this document\n",
    "## Alpha and beta are current parameter values\n",
    "## doc is the current document\n",
    "def Estep(K, N, V, alpha, beta, doc, tol):    \n",
    "        \n",
    "    #initialize phi and gamma\n",
    "    oldPhi  = np.full(shape = (N,K), fill_value = 1/K)\n",
    "    gamma = alpha + N/K\n",
    "    converge = 0 \n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    while converge == 0:\n",
    "        newPhi  = np.zeros(shape = (N,K))\n",
    "        #Update phi\n",
    "        for n in range(N):\n",
    "            ind = np.where(doc[n,:] == 1)[0]\n",
    "            newPhi[n,:] = [beta[i, ind]*np.exp(special.psi(gamma[i])) for i in range(K)]\n",
    "        newPhi = newPhi/np.sum(newPhi, axis = 1)[:, np.newaxis] #normalizing the rows of new phi\n",
    "\n",
    "        gamma = alpha + np.sum(newPhi, axis = 0)  #updating gamma\n",
    "\n",
    "        #Check convergence criteria\n",
    "        criteria = (1/(N*K))*np.sum((newPhi - oldPhi)**2)**0.5\n",
    "        if criteria < tol:\n",
    "            converge = 1\n",
    "        else:\n",
    "            oldPhi = newPhi\n",
    "            count = count +1\n",
    "            converge = 0\n",
    "    return (newPhi, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Update alpha using linear Newton-Rhapson Method#\n",
    "## Gamma is a M x K array now\n",
    "def alphaUpdate(K, M, alphaOld, gamma, tol):\n",
    "    h = np.zeros(K)\n",
    "    g = np.zeros(K)\n",
    "    alphaNew = np.zeros(K)\n",
    "\n",
    "    step = np.zeros(K)\n",
    "    \n",
    "    \n",
    "    converge = 0\n",
    "    while converge == 0:\n",
    "        \n",
    "        alpha_sum = np.sum(alphaOld)\n",
    "        z = -special.polygamma(1, alpha_sum)\n",
    "        h = M*special.polygamma(1, alphaOld)\n",
    "        \n",
    "        gam_sum = [special.psi(np.sum(gamma[d, :])) for d in range(M)]\n",
    "        doc_sum = special.psi(np.sum(gamma, axis = 0)) - np.sum(gam_sum)\n",
    "        g = np.array([M*(special.psi(alpha_sum)-special.psi(alphaOld[i])) + doc_sum[i] for i in range(K)])\n",
    "        \n",
    "        \n",
    "        c = np.sum(g/h)/(1/z + np.sum(1/h))\n",
    "        step = (g-c)/h\n",
    "        \n",
    "        # step = (g - c)/h\n",
    "\n",
    "        if np.linalg.norm(step) < tol:\n",
    "            converge = 1\n",
    "        else:\n",
    "            converge = 0\n",
    "            alphaNew = alphaOld + step\n",
    "            alphaOld = alphaNew\n",
    "\n",
    "    return abs(alphaNew)\n",
    "\n",
    "# V is the number of words in the vocabulary\n",
    "# phi_list is a list of the phi matrices, one for each document\n",
    "# Word list is a list of the word matrix for each document\n",
    "def betaUpdate(K, M, V, phi_list, word_list):\n",
    "\n",
    "    \n",
    "    #Calculate beta#\n",
    "    beta = np.zeros(shape = (K,V))\n",
    "    Nd = [word_list[d].shape[0] for d in range(M)] # Number of words for document d\n",
    "\n",
    "    for i in range(K):\n",
    "        for j in range(V):\n",
    "            wordSum = np.sum([phi_list[d][n,i]*word_list[d][n,j] for d in range(M) for n in range(Nd[d])])\n",
    "            beta[i,j] = wordSum\n",
    "    \n",
    "    \n",
    "    #Normalize the rows of beta#\n",
    "    beta = beta/np.sum(beta, axis = 1)[:, np.newaxis]\n",
    "\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K = number of topics \n",
    "# M = number of documents\n",
    "# corpus matrix is output of make_word_matrix\n",
    "# tol is the desired tolerance for convergence\n",
    "def LDA(K, M, corpusMatrix, tol):\n",
    "\n",
    "    M = corpusMatrix[2]\n",
    "    output = []\n",
    "    \n",
    "    converge = 0\n",
    "    #initialize alpha and beta for first iteration\n",
    "    alphaOld = np.full(shape = K, fill_value = 50/K) + np.random.rand(K)\n",
    "    V = corpusMatrix[0][0].shape[1]\n",
    "    betaOld = np.random.rand(K, V)\n",
    "    betaOld = betaOld/np.sum(betaOld, axis = 1)[:, np.newaxis]\n",
    "    \n",
    "    iteration = 0\n",
    "    while converge == 0:\n",
    "        phi = []\n",
    "        gamma = []\n",
    "        \n",
    "        ##E-step##\n",
    "        #looping through the number of documents\n",
    "        ## Perform LDA E-step for one document\n",
    "        for d in range(M):\n",
    "            N = corpusMatrix[0][d].shape[0]\n",
    "            phiT, gammaT = Estep(K, N, V, alphaOld, betaOld, corpusMatrix[0][d], tol)\n",
    "            phi.append(phiT)\n",
    "            gamma.append(gammaT)\n",
    "        print(\"E-step done\")    \n",
    "        ##M - step##\n",
    "        #Update alpha and beta#     \n",
    "        gamma = np.array(gamma)\n",
    "        alphaNew = alphaUpdate(K, M, alphaOld, gamma, 1)\n",
    "        betaNew = betaUpdate(K, M, V, phi, corpusMatrix[0])\n",
    "        print(\"M-step done\")\n",
    "    \n",
    "        if np.linalg.norm(alphaOld - alphaNew) < tol or np.linalg.norm(betaOld - betaNew) < tol:\n",
    "            converge =1\n",
    "        else: \n",
    "            converge =0\n",
    "            iteration += 1\n",
    "            alphaOld = alphaNew\n",
    "            betaOld = betaNew\n",
    "            print(iteration)\n",
    "    output.append([phi, gamma, alphaNew, betaNew])\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E-step done\n",
      "M-step done\n",
      "1\n",
      "E-step done\n",
      "M-step done\n",
      "2\n",
      "E-step done\n",
      "M-step done\n"
     ]
    }
   ],
   "source": [
    "K = 3\n",
    "M = testMat[2]\n",
    "tol = 0.01\n",
    "toy = LDA(K, M, testMat, tol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#function to return the most probable words\n",
    "#p is the number of words you want returned for each topic\n",
    "def mostCommon(beta, wordList, p):\n",
    "    k = beta.shape[0]\n",
    "    topicWords = []\n",
    "    betaDF = pd.DataFrame(beta)\n",
    "    for i in range(0, k):\n",
    "        document = betaDF.loc[i,:]\n",
    "        ind = np.array(betaDF.iloc[0].sort_values(ascending = False).index[:p])\n",
    "        mostCommon = pd.DataFrame(np.array(wordList)[ind])\n",
    "        topicWords.append(mostCommon)\n",
    "    return(topicWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_topics = mostCommon(toy[0][3], testMat[1], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>brown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fox</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>friend</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0\n",
       "0   brown\n",
       "1     fox\n",
       "2  friend"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_topics[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Blei, David M., Ng, Andrew Y. and Jordan, Michael I. *Latent Dirichlet Allocation*. Journal of Machine Learning Research 3 (2003) 993-1022. http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
